## 特征工程

### 一、特征选择 Feature Selection

 **refer: https://blog.csdn.net/zhenaoxi1077/article/details/82791281**

我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量

特征选择，它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。

特征选择的方法：

 - Filter 过滤法
   - 方差选择法： 先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。
   - 相关系数法（Pearson系数）
     - 使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。
     - 主要用在特征是连续变量，target也为连续变量的情况
   - 卡方检验
     - 检验定性自变量与定型因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距
   - 互信息法
     - 经典的互信息也是评价定性自变量对定性因变量的相关性的
 - Wrapper
   - 循环特征消除法（RFE）
     - 使用一个基模型进行多轮训练，每次训练后，消除若干系数比较小的特征，在基于新的特征集进行下一轮计算
 - Embedded
   - 基于惩罚项的特征选择法
     - 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。
     - L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型
   - 基于树模型的特征选择法
     - **refer:https://blog.csdn.net/u014035615/article/details/79612827**
     - 随机森林（Random Forest）
       - 用袋外数据 (OOB) 做预测。随机森林在每次重抽样建立决策树时，都会有一些样本没有被选中，那么就可以用这些样本去做交叉验证，这也是随机森林的优点之一。它可以不用做交叉验证，直接用oob _score_去对模型性能进行评估。
       - 具体的方法就是：
         1. 对于每一棵决策树，用OOB 计算袋外数据误差，记为 errOOB1；
         2. 然后随机对OOB所有样本的特征i加入噪声干扰，再次计算袋外数据误差，记为errOOB2；
         3. 假设有N棵树，特征i的重要性为sum(errOOB2-errOOB1)/N;
       - 如果加入随机噪声后，袋外数据准确率大幅下降，说明这个特征对预测结果有很大的影响，进而说明它的重要程度比较高
      - 梯度提升树（GBDT）
        - 主要是通过计算特征i在单棵树中重要度的平均值
        - 特征i在单棵树的重要度主要是通过计算按这个特征i分裂之后损失的减少值
     - XGboost
       - XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+...)。


