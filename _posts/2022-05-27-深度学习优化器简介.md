---
title: 深度学习优化器
tags: [优化器, SGD, Adagrad, Adam]
---

# 1. 简介

对比深度学习中的各种优化器

# 2. 优化器介绍

## 2.1. SGD

SGD即mini-batch gradient descent. 就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法。

$$g_t = \Delta_{\theta_t} L(\theta_{t-1})$$

$$\Delta \theta_t = -\eta * g_t$$

其中，$\eta$ 是学习率，$g_t$ 是梯度 SGD完全依赖于当前batch的梯度，所以 $\eta$ 可理解为允许当前batch的梯度多大程度影响参数更新

### 2.1.1. 缺点
 - 选择合适的learning rate比较困难, 对所有的参数更新使用同样的learning rate
 - SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点

## 2.2. 动量法

动量发是模拟物理里动量的概念，积累之前的动量来替代当前真正的梯度。

$$m_t = \mu * m_{t-1} + g_t$$

$$\Delta \theta_t = -\eta * m_t$$

### 2.2.1. 特点

 - 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的 $\mu$ 能够进行很好的加速
 - 下降中后期时，在局部最小值来回震荡的时候，梯度趋近于0， $\mu$ 使得更新幅度增大，跳出局部最优点

## 2.3. Adagrad

Adagrad其实是对学习率进行了一个约束

$$\Delta \theta_t = - \frac{\eta}{\sqrt{\sum_{r=1}^t(g_r)^2 + \epsilon}} * g_t$$

分母中，$\sqrt{\sum_{r=1}^t(g_r)^2}$ 表示了前t 步参数θi梯度的平方累加;
ϵ一般是一个极小值，作用是防止分母为0

### 2.3.1. 特点

 - 训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]

 - 训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]
 - 适合处理稀疏梯度

### 2.3.2. 缺点
 - 由公式可以看出，仍依赖于人工设置一个全局学习率
 - $\eta$ 设置过大的话，会使regularizer过于敏感，对梯度的调节太大
 - 中后期，分母上梯度平方的累加将会越来越大，使[公式]，使得训练提前结束


## 2.4. Adadelta

Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。

$$n_t = \nu * n_{t-1} + (1-\nu) * {g_t}^2$$

$$\Delta \theta_t = - \frac{\eta}{\sqrt{n_t + \epsilon}} * g_t$$

在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后：

$$E|g^2|_t = \rho * {E|g^2|_{t-1}} + (1 - \rho) * {g_t}^2$$

$$\Delta x_t = - \frac{\sqrt{\sum_{r=1}^{t-1}\Delta x_r}}{\sqrt{E|g^2|_t + \epsilon}}$$

其中，$E$代表求期望。

此时，可以看出Adadelta已经不用依赖于全局学习率了。

## 2.5. RMSprop

RMSprop可以算作Adadelta的一个特例：

当$\rho = 0.5$时，
$$E|g^2|_t = \rho * E|g^2|_{t-1} + (1 - \rho) * {g_t}^2$$ 
就变为了求梯度平方和的平均数。

如果再求根的话，就变成了RMS(均方根)：

$$RMS\|g\|_t = \sqrt{E\|g^2\|_t + \epsilon}$$

此时，这个RMS就可以作为学习率 $\eta$ 的一个约束：

$$\Delta x_t = - \frac{\eta}{RMS\|g\|_t} * g_t$$

### 2.5.1. 特点
 - 其实RMSprop依然依赖于全局学习率
 - RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
 - 适合处理非平稳目标 - 对于RNN效果很好

## 2.6. Adam
Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。

### 2.6.1. 特点

 - 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
 - 对内存需求较小
 - 为不同的参数计算不同的自适应学习率
 - 也适用于大多非凸优化 - 适用于大数据集和高维空间

